<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>hadoop2.6+spark1.6.3分布式计算框架的搭建 | 麦桃子的部落格</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="软硬件设备及安装前的环境搭建
操作系统：centos 6.7
软件版本：hadoop－2.6.0 ， spark－1.6.3 ， jdk1.8.0_111 ,scala-2.11.6
集群架构 ：包括三个节点，1个master，2个slave,节点之间局域网连接，可以互相ping通，节点IP地址分布如下:
master   10.108.101.210
slave1   10.108.103.13">
<meta property="og:type" content="article">
<meta property="og:title" content="hadoop2.6+spark1.6.3分布式计算框架的搭建">
<meta property="og:url" content="http://yoursite.com/2017/01/13/hadoop2-6-spark1-6-3分布式计算框架的搭建/index.html">
<meta property="og:site_name" content="麦桃子的部落格">
<meta property="og:description" content="软硬件设备及安装前的环境搭建
操作系统：centos 6.7
软件版本：hadoop－2.6.0 ， spark－1.6.3 ， jdk1.8.0_111 ,scala-2.11.6
集群架构 ：包括三个节点，1个master，2个slave,节点之间局域网连接，可以互相ping通，节点IP地址分布如下:
master   10.108.101.210
slave1   10.108.103.13">
<meta property="og:image" content="http://7xruqa.com1.z0.glb.clouddn.com/spark%20on%20yarn%E8%BF%90%E8%A1%8C%E5%9B%BE.jpg">
<meta property="og:updated_time" content="2017-01-14T07:21:19.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="hadoop2.6+spark1.6.3分布式计算框架的搭建">
<meta name="twitter:description" content="软硬件设备及安装前的环境搭建
操作系统：centos 6.7
软件版本：hadoop－2.6.0 ， spark－1.6.3 ， jdk1.8.0_111 ,scala-2.11.6
集群架构 ：包括三个节点，1个master，2个slave,节点之间局域网连接，可以互相ping通，节点IP地址分布如下:
master   10.108.101.210
slave1   10.108.103.13">
<meta name="twitter:image" content="http://7xruqa.com1.z0.glb.clouddn.com/spark%20on%20yarn%E8%BF%90%E8%A1%8C%E5%9B%BE.jpg">
  
    <link rel="alternative" href="/atom.xml" title="麦桃子的部落格" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://7xruqa.com1.z0.glb.clouddn.com/IMG_myphoto.JPG" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Mai Taozi</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						
						<li>Über</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="#" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
					        
								<a class="rss" target="_blank" href="#" title="rss">rss</a>
					        
								<a class="zhihu" target="_blank" href="#" title="zhihu">zhihu</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">越努力 越幸运</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Mai Taozi</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="http://7xruqa.com1.z0.glb.clouddn.com/IMG_myphoto.JPG" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">Mai Taozi</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="#" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
			        
						<a class="rss" target="_blank" href="#" title="rss">rss</a>
			        
						<a class="zhihu" target="_blank" href="#" title="zhihu">zhihu</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap"><article id="post-hadoop2-6-spark1-6-3分布式计算框架的搭建" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/01/13/hadoop2-6-spark1-6-3分布式计算框架的搭建/" class="article-date">
  	<time datetime="2017-01-13T02:29:25.000Z" itemprop="datePublished">2017-01-13</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      hadoop2.6+spark1.6.3分布式计算框架的搭建
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
        

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="软硬件设备及安装前的环境搭建"><a href="#软硬件设备及安装前的环境搭建" class="headerlink" title="软硬件设备及安装前的环境搭建"></a>软硬件设备及安装前的环境搭建</h2><blockquote>
<p>操作系统：centos 6.7</p>
<p>软件版本：hadoop－2.6.0 ， spark－1.6.3 ， jdk1.8.0_111 ,scala-2.11.6</p>
<p>集群架构 ：包括三个节点，1个master，2个slave,节点之间局域网连接，可以互相ping通，节点IP地址分布如下:</p>
<pre><code>master   10.108.101.210
slave1   10.108.103.137
slave2   10.108.100.59
</code></pre></blockquote>
<h3 id="1-jdk-1-8-0-111安装"><a href="#1-jdk-1-8-0-111安装" class="headerlink" title="1.jdk-1.8.0_111安装"></a>1.jdk-1.8.0_111安装</h3><p>官网下载最新的的jdk tar.gz到/home目录下，在usr目录下建立java安装目录，解压</p>
<pre><code>cd /usr
mkdir java
tar zxvf /home/jdk-8u111-linux-x64.tar.gz -C /usr/java/
</code></pre><p>得到文件夹 jdk1.8.0_111</p>
<p>安装完毕为他建立一个链接以节省目录长度</p>
<pre><code>ln -s /usr/java/jdk1.8.0_111/ /usr/jdk
</code></pre><p>编辑配置文件，配置环境变量</p>
<pre><code>JAVA_HOME=/usr/jdk
CLASSPATH=.:$JAVA_HOME/lib/
PATH=$PATH:$JAVA_HOME/bin
export PATH JAVA_HOME CLASSPATH
</code></pre><p>重启机器或执行命令：source /etc/profile</p>
<p>查看安装情况： java －version </p>
<pre><code>java version &quot;1.8.0_111&quot;
Java(TM) SE Runtime Environment (build 1.8.0_111-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode)
</code></pre><p>卸载本机jdk</p>
<blockquote>
<p>rpm -qa|grep jdk</p>
<p>yum -y remove java-1.7.0-openjdk</p>
</blockquote>
<h3 id="2-修改-etc-hosts"><a href="#2-修改-etc-hosts" class="headerlink" title="2.修改/etc/hosts"></a>2.修改/etc/hosts</h3><p>追加</p>
<pre><code>10.108.101.210   master
10.108.103.137   slave1
10.108.100.59    slave2
</code></pre><h3 id="3-配置ssh，实现免密码登陆"><a href="#3-配置ssh，实现免密码登陆" class="headerlink" title="3.配置ssh，实现免密码登陆"></a>3.配置ssh，实现免密码登陆</h3><p>在所用主机上新建hadoop用户</p>
<pre><code>#useradd hadoop
#passwod hadoop
</code></pre><p>关闭所有主机的防火墙</p>
<pre><code>service iptables stop
</code></pre><p>切换至hadoop用户</p>
<pre><code>su hadoop
</code></pre><p>在三台机子上执行,用hadoop登陆系统，进入个人主目录 /home/hadoop</p>
<pre><code>ssh-keygen -t rsa -P &apos;&apos;  //设置ssh的密钥和密钥存放路径，路径为～/.ssh下
</code></pre><p>以rsa算法，生成公钥，私钥，－P ‘’ 表示是空密码，该命令执行完成后，会在个人目录下生产.ssh目录，里面会有两个文件，id_rsa(私钥)和id_rsa(公钥)，该目录为隐藏目录，可用一下目录查看</p>
<pre><code>ls -lah
</code></pre><p>在master上将公钥放到authrozied_keys里</p>
<pre><code>sudo cat id_rsa.pub &gt;&gt; authorized_keys
chmod 600 authorized_keys
</code></pre><p>用scp将公钥文件拷贝到master机器上</p>
<pre><code>slave1: scp .ssh/id_rsa.pub hadoop@master:/home/hadoop/id_rsa_01.pub
slave2: scp .ssh/id_rsa.pub hadoop@master:/home/hadoop/id_rsa_02.pub
</code></pre><p>执行后，在master机器上，查看/home/hadoop目录，里面会有id_rsa_01和id_rsa_02.pub两个文件，然后再master机器上倒入这两个公钥</p>
<pre><code>cat id_rsa_01.pub &gt;&gt; .ssh/authorized_keys
cat id_rsa_02.pub &gt;&gt; .ssh/authorized_keys
</code></pre><p>将master上的公钥，复制到其他机器上，在master机器执行一下命令</p>
<pre><code>scp .ssh/authorized_keys hadoop@slave1:/home/hadoop/.ssh/authorized_keys
scp .ssh/authorized_keys hadoop@slave2:/home/hadoop/.ssh/authorized_keys
</code></pre><p>修改三台机子authorized_keys权限</p>
<pre><code>chmod 644 authorized_keys
</code></pre><p>测试是否成功：在任意一台机子上ssh hadoop@slave1不用密码直接进入系统，这就表示成功了。</p>
<p>如果hadoop用户使用不了sudo指令：</p>
<pre><code>vi /etc/sudoers
hadoop  ALL=(root)   NOPASSWD:ALL   //在root  ALL=(ALL)  ALL后添加这一句
</code></pre><h2 id="hadoop上传安装与配置"><a href="#hadoop上传安装与配置" class="headerlink" title="hadoop上传安装与配置"></a>hadoop上传安装与配置</h2><h3 id="1-下载解压hadoop"><a href="#1-下载解压hadoop" class="headerlink" title="1.下载解压hadoop"></a>1.下载解压hadoop</h3><p>把hadoop上传到服务器并解压到/usr/hadoop目录中</p>
<pre><code>tar zxvf hadoop-2.6.0.tar.gz -C /usr/hadoop
</code></pre><p>切换到/usr/hadoop目录下，修改hadoop-2.6.0的所有者</p>
<pre><code>chown -R hadoop.hadoop hadoop-2.6.0
</code></pre><h3 id="2-配置hadoop环境变量"><a href="#2-配置hadoop环境变量" class="headerlink" title="2.配置hadoop环境变量"></a>2.配置hadoop环境变量</h3><p>vi /etc/profile 配置环境变量</p>
<pre><code>HADOOP_HOME=/usr/hadoop/hadoop-2.6.0
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
</code></pre><h3 id="3-hadoop配置"><a href="#3-hadoop配置" class="headerlink" title="3.hadoop配置"></a>3.hadoop配置</h3><p>位置：$HADOOP_HOME/etc/hadoop</p>
<h4 id="1-hadoop-env-sh和yarn-env-sh-加入"><a href="#1-hadoop-env-sh和yarn-env-sh-加入" class="headerlink" title="1.hadoop-env.sh和yarn-env.sh 加入"></a>1.hadoop-env.sh和yarn-env.sh 加入</h4><pre><code>export JAVA_HOME=/usr/jdk
</code></pre><h4 id="2-core-site-xml"><a href="#2-core-site-xml" class="headerlink" title="2.core-site.xml"></a>2.core-site.xml</h4><pre><code>&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
&lt;value&gt;file:/usr/hadoop/hadoop-2.6.0/tmp&lt;/value&gt;
&lt;description&gt;Abase for other temporary directories.&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;fs.default.name&lt;/name&gt;
 &lt;value&gt;hdfs://master:9000&lt;/value&gt;
&lt;/property&gt;

&lt;/configuration&gt;
</code></pre><p>注意：/home/hadoop/tmp，这个文件要手动提前创建好;9000端口要开放，否则会出现hadoop正常启动，但看不到datanode使用的情况</p>
<h4 id="3-hdfs-site-xml"><a href="#3-hdfs-site-xml" class="headerlink" title="3.hdfs-site.xml"></a>3.hdfs-site.xml</h4><pre><code>&lt;configuration&gt;
&lt;property&gt;
  &lt;property&gt; 
     &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
  &lt;value&gt;master:50090&lt;/value&gt;
&lt;/property&gt;
  &lt;property&gt;
     &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
     &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
     &lt;value&gt;file:/usr/hadoop/hadoop-2.6.0/tmp/dfs/name&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
     &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
     &lt;value&gt;file:/usr/hadoop/hadoop-2.6.0/tmp/dfs/data&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
     &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
     &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.nameservices&lt;/name&gt;
    &lt;value&gt;hadoop-cluster1&lt;/value&gt;
 &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><p>注意：/home/hadoop/hadoop-2.6.0/dfs/name 和 /home/hadoop/hadoop-2.6.0/dfs/data，这两个文件夹要手动创建</p>
<h4 id="4-mapred-site-xml"><a href="#4-mapred-site-xml" class="headerlink" title="4.mapred-site.xml"></a>4.mapred-site.xml</h4><pre><code>&lt;configuration&gt;
&lt;property&gt;
            &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
            &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
            &lt;value&gt;master:10020&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
            &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
            &lt;value&gt;master:19888&lt;/value&gt;
    &lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt;
    &lt;value&gt;master:50030&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
            &lt;name&gt;mapred.job.tracker&lt;/name&gt;
            &lt;value&gt;http://master:9001&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre><h4 id="5-yarn-site-xml"><a href="#5-yarn-site-xml" class="headerlink" title="5.yarn-site.xml"></a>5.yarn-site.xml</h4><pre><code>&lt;configuration&gt;

&lt;!-- Site specific YARN configuration properties --&gt;

&lt;property&gt;
     &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
     &lt;value&gt;master&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
    &lt;value&gt;master:8032&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
    &lt;value&gt;master:8030&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
    &lt;value&gt;master:8031&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
    &lt;value&gt;master:8033&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
    &lt;value&gt;master:8088&lt;/value&gt;
&lt;/property&gt;

&lt;/configuration&gt;
</code></pre><h4 id="6-slaves"><a href="#6-slaves" class="headerlink" title="6.slaves"></a>6.slaves</h4><pre><code>master
slave1
slave2
</code></pre><h3 id="4-将master上配置好的hadoop目录拷贝到slave1，slave2"><a href="#4-将master上配置好的hadoop目录拷贝到slave1，slave2" class="headerlink" title="4.将master上配置好的hadoop目录拷贝到slave1，slave2"></a>4.将master上配置好的hadoop目录拷贝到slave1，slave2</h3><pre><code>scp -r /usr/hadoop/hadoop-2.6.0 hadoop@slave1:/usr/hadoop/
scp -r /usr/hadoop/hadoop-2.6.0 hadoop@slave2:/usr/hadoop/
</code></pre><h3 id="5-验证"><a href="#5-验证" class="headerlink" title="5.验证"></a>5.验证</h3><p>master机器上，进入到hadoop根目录</p>
<p>格式化    </p>
<pre><code>bin/hdfs namenode -format
</code></pre><p>启动hadoop</p>
<pre><code>sbin/start-all.sh
</code></pre><p>查看是否正常，通过jps命令如果有一下内容说明ok</p>
<p>master下有：</p>
<blockquote>
<p>ResourceManager</p>
<p>NameNode</p>
<p>SecondaryNameNode</p>
</blockquote>
<p>slave下有：</p>
<blockquote>
<p>DataNode</p>
<p>NodeManager</p>
</blockquote>
<p>浏览器访问：http:master:50070/ 和 <a href="http://master:8088" target="_blank" rel="external">http://master:8088</a></p>
<p>查看状态：bin/hdfs dfsadmin -report</p>
<h2 id="安装scala"><a href="#安装scala" class="headerlink" title="安装scala"></a>安装scala</h2><p><a href="http://www.scala-lang.org/download/2.11.6.html" target="_blank" rel="external">scala下载</a></p>
<p>切换至root用户</p>
<pre><code>sudo su
</code></pre><p>创建/usr/scala文件夹</p>
<pre><code>mkdir /usr/scala
</code></pre><p>将压缩包解压至/usr/scala目录</p>
<pre><code>tar zxvf /opt/scala-2.11.6.tgz -C /usr/scala
</code></pre><p>配置环境变量</p>
<pre><code>vi /etc/profile
#追加如下内容
export SCALA_HOME=/usr/scala/scala-2.11.6
export PATH=$PATH:$SCALA_HOME/bin
</code></pre><p>使环境变量生效</p>
<pre><code>source /etc/profile
</code></pre><p>测试环境变量设置</p>
<pre><code>scala -version
</code></pre><h2 id="Spark安装与配置"><a href="#Spark安装与配置" class="headerlink" title="Spark安装与配置"></a>Spark安装与配置</h2><h3 id="1-spark下载与解压"><a href="#1-spark下载与解压" class="headerlink" title="1.spark下载与解压"></a>1.spark下载与解压</h3><p><a href="http://www.apache.org/dyn/closer.lua/spark/spark1.6.3/spark-1.6.3-bin-hadoop2.6.tgz" target="_blank" rel="external">Spark下载</a></p>
<p>将压缩包解压至/usr/spark目录</p>
<pre><code>tar zxvf /opt/spark1.6.3-bin-hadoop2.6.tgz -C /usr/spark
</code></pre><p>将spark文件夹授权给hadoop用户</p>
<pre><code>chown -R hadoop:hadoop /usr/spark/
</code></pre><h3 id="2-设置环境变量"><a href="#2-设置环境变量" class="headerlink" title="2.设置环境变量"></a>2.设置环境变量</h3><pre><code>vi /etc/profile
#追加如下内容
export SPARK_HOME=/usr/spark/spark-1.6.3-bin-hadoop2.6
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
</code></pre><p>使环境变量生效</p>
<pre><code>source /etc/profile
</code></pre><p>测试环境变量设置</p>
<pre><code>spark-shell --version
</code></pre><p>运行SparkPi</p>
<pre><code>run-example org.apache.spark.examples.SparkPi 10
</code></pre><h3 id="3-配置Spark"><a href="#3-配置Spark" class="headerlink" title="3.配置Spark"></a>3.配置Spark</h3><p>切换至hadoop用户</p>
<pre><code>su hadoop
</code></pre><p>修改spark-env.sh</p>
<pre><code>cd /usr/spark/spark-1.6.3-bin-hadoop2.6/conf
cp spark-env.sh.template spark-env.sh
vi spark-env.sh
#追加如下内容
export SCALA_HOME=/usr/scala/scala-2.12.0
export JAVA_HOME=/usr/jdk
export SPARK_MASTER_IP=10.108.101.27
export SPARK_WORKEY_MOMORY=1024m
</code></pre><p>启动spark</p>
<pre><code>/usr/spark/spark-1.6.3-bin-hadoop2.6/sbin/start-all.sh
#停止命令
/usr/spark/spark-1.6.3-bin-hadoop2.6/sbin/stop-all.sh

#另一种方式
start-master.sh
start-slave.sh spark://10.108.101.210:7077
</code></pre><p>提交任务到spark集群</p>
<pre><code>spark-submit --master spark://master:7077 --class org.apache.spark.examples.SparkPi --name Spark-Pi /usr/spark/spark-1.6.3-bin-hadoop2.6/lib/spark-examples-1.6.3-hadoop2.6.0.jar
</code></pre><p>使用web查看Spark运行状态</p>
<h3 id="4-在Yarn中执行Spark任务"><a href="#4-在Yarn中执行Spark任务" class="headerlink" title="4.在Yarn中执行Spark任务"></a>4.在Yarn中执行Spark任务</h3><p>编辑spark-env.sh</p>
<pre><code>vi /usr/spark/spark-1.6.3-bin-hadoop2.6/conf/spark-env.sh
#追加如下内容
export HADOOP_CONF_DIR=/usr/hadoop/hadoop-2.6.0/etc/hadoop
</code></pre><p>运行SparkLR程序，提交Spark任务到yarn中</p>
<pre><code>spark-submit --master yarn-cluster --class org.apache.spark.examples.SparkLR --name SparkLR /usr/spark/spark-1.6.3-bin-hadoop2.6/lib/spark-examples-1.6.3-hadoop2.6.0.jar
</code></pre><p>使用web查看任务运行状态</p>
<pre><code>http://master:8088/
</code></pre><p><img src="http://7xruqa.com1.z0.glb.clouddn.com/spark%20on%20yarn%E8%BF%90%E8%A1%8C%E5%9B%BE.jpg" alt=""></p>
<p>在Yarn中结合HDFS运行Spark任务</p>
<p>运行JavaWordCount程序</p>
<pre><code>spark-submit --master yarn-cluster --class org.apache.spark.examples.JavaWordCount --name JavaWordCount /usr/spark/spark-1.6.3-bin/hadoop2.6/lib/spark-examples-1.6.3-hadoop2.6.0.jar hdfs://master:9000/user/hadoop/input.txt
</code></pre><font color="red">注意:hdfs://master:9000/user/hadoop/input是输入文件的地址，把需要进行wordcount的数据上传到hdfs文件系统当中</font>

<blockquote>
<p>bin/hadoop dfs -ls    //列出HDFS下的文件</p>
<p>bin/hadoop dfs -ls in //列出HDFS文件下名为in的文档中的文件</p>
<p>bin/hadoop dfs test1 test  //将hadoop目录下的test1文件上传到HDFS上并重命名为test</p>
<p>bin/hadoop dfs -get in getin   //将HDFS中的in文件复制到本地系统并命名为getin</p>
<p>bin/hadoop dfs -cat in/*   //查看HDFS下in文件中的内容</p>
</blockquote>
<p>查看运行结果，因为该示例程序输出结果到控制台，所以我们去查看控制台日志</p>
<blockquote>
<p>1.点击History（上图中红色框尾部）</p>
<p>2.点击Logs（位置在右下部，与history类似）</p>
<p>3.点击stdout:Total file length is XXXXX bytes.</p>
</blockquote>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/01/14/Kafka集群搭建/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption"><</strong>
      <div class="article-nav-title">
        
          Kafka集群搭建
        
      </div>
    </a>
  
  
    <a href="/2016/10/09/javaweb开发之Servlet总结/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">javaweb开发之Servlet总结</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>

  
</article>


<div class="share_jia">
	<!-- JiaThis Button BEGIN -->
	<div class="jiathis_style">
		<span class="jiathis_txt">Share to: &nbsp; </span>
		<a class="jiathis_button_facebook"></a> 
    <a class="jiathis_button_twitter"></a>
    <a class="jiathis_button_plus"></a> 
    <a class="jiathis_button_tsina"></a>
		<a class="jiathis_button_cqq"></a>
		<a class="jiathis_button_douban"></a>
		<a class="jiathis_button_weixin"></a>
		<a class="jiathis_button_tumblr"></a>
    <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	</div>
	<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1405949716054953" charset="utf-8"></script>
	<!-- JiaThis Button END -->
</div>






<div class="duoshuo">
	<!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="hadoop2-6-spark1-6-3分布式计算框架的搭建" data-title="hadoop2.6+spark1.6.3分布式计算框架的搭建" data-url="http://yoursite.com/2017/01/13/hadoop2-6-spark1-6-3分布式计算框架的搭建/"></div>
	<!-- 多说评论框 end -->
	<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"true"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<!-- 多说公共JS代码 end -->
</div>




</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2017 Mai Taozi
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>